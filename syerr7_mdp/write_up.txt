Name: Yerramsetty Sudha Sree
UIN: 662518247
Assignment - 6 (using sample input provided by Lecturer)

Markov Decision Process for Grid World:
This program implements a Markov Decision Process (MDP) for a stochastic grid world environment, allowing for value iteration and modified policy iteration methods. 
The grid world consists of states, actions, transitions, rewards, and terminal states, which are defined in an input text file. 
The program computes the value of each state and the optimal policy, printing the results to specified output files.

How to Run the Program:
    1)  Before running the script, make sure you have Python installed.
        If not, download Python from https://www.python.org/downloads/.
        Ensure you install Python 3.8 or higher.
        Make sure PATH environment variables are set to Python312 and Python312 Scripts. (for Python 3.12.4 which is the one I'm using)
    2)  Extract the files from <netid>_mdp.zip into a folder.
    3)  Open Terminal/Command Prompt and navigate to the directory where the file is located using cd.
    4)  Make sure that you already have an input.txt file in the folder. 
        It should be similar to the one provided on Blackboard in CS411 Assignment 6 input.
        If required, inputs can be modified in the input.txt file.
    5)  Run the file using the command: "python mdp.py"
    6)  After the program has finished running, 
        it will output the policy and utility values of every iteration for value iteration in "output_value_iteration.txt", 
        and the policy and utility values of every iteration for modified policy iteration in "output_modified_policy_iteration.txt".

        NOTE: Iterations are limited to 100 in modified policy iteration. You can increase it if required.


Outputs:
The program generates two output files:
    1)  output_value_iteration.txt: Contains the utility values of states in each iteration of the value iteration process and the final policy.
    2)  output_modified_policy_iteration.txt: Contains the utility values of states in each iteration of the modified policy iteration process and the final policy.

Formats of each file (just for the sake of clarity):
    1)  input file => text file => Ex: input.txt
    2)  output files => should be text files => Ex: output.txt
    3)  MDP implementation => Python file => Ex: mdp.py

Description of Input Fields in Input File:
    1)  size: Dimensions of the grid.
    2)  walls: A list of positions where walls are located.
    3)  terminal_states: A list of terminal states or end states, after reaching which, the agent can move no further.
    4)  reward: The reward for non-terminal states.
    5)  transition_probabilities: Specifies the probabilities for the intended direction and the probabilities for moving at right angles to the intended direction. 
        For example, 0.8 0.1 0.1 0 means there is an 80% chance to move in the intended direction and 10% to move in each of the two perpendicular directions to the intended direction.
    6)  discount_rate: The discount factor for future rewards, typically between 0 and 1. 1 indicates a purely additive utility function.
    7)  epsilon: A small threshold value for convergence criteria in iterative algorithms. 
        For example, 0.001 means that the algorithm will stop iterating when the change in values is less than 0.001.
